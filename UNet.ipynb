{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch, os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for root,dirs,files in os.walk('./seg/img'):\n",
    "    if files:\n",
    "        lst = os.listdir(root)\n",
    "        png = fnmatch.filter(lst,'*.png')\n",
    "        if png:\n",
    "            for i in png:\n",
    "                train.append((root+'/'+i, '/'.join(root.split('/')[:2])+'/mask/'+i))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_path, transform = None, transform_label = None):\n",
    "        self.image_list = image_path\n",
    "        self.len = len(self.image_list)\n",
    "        self.transform = transform\n",
    "        self.transform_label = transform_label\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.image_list[index]\n",
    "        img = Image.open(fn)\n",
    "        label = Image.open(label)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) \n",
    "            label = self.transform_label(label)\n",
    "        return img, label\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "Transform = transforms.Compose([\n",
    "    #transforms.RandomRotation(degrees=15),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])\n",
    "Lable_transfrom = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    " \n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DiceLoss, self).__init__()\n",
    " \n",
    "\tdef\tforward(self, input, target):\n",
    "\t\tN = target.size(0)\n",
    "\t\tsmooth = 1\n",
    " \n",
    "\t\tinput_flat = input.view(N, -1)\n",
    "\t\ttarget_flat = target.view(N, -1)\n",
    " \n",
    "\t\tintersection = input_flat * target_flat\n",
    " \n",
    "\t\tloss = 2 * (intersection.sum(1) + smooth) / (input_flat.sum(1) + target_flat.sum(1) + smooth)\n",
    "\t\tloss = 1 - loss.sum() / N\n",
    " \n",
    "\t\treturn loss\n",
    " \n",
    "class MulticlassDiceLoss(nn.Module):\n",
    "\t\"\"\"\n",
    "\trequires one hot encoded target. Applies DiceLoss on each class iteratively.\n",
    "\trequires input.shape[0:1] and target.shape[0:1] to be (N, C) where N is\n",
    "\t  batch size and C is number of classes\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MulticlassDiceLoss, self).__init__()\n",
    " \n",
    "\tdef forward(self, input, target):\n",
    " \n",
    "\t\tC = target.shape[1]\n",
    " \n",
    "\t\t# if weights is None:\n",
    "\t\t# \tweights = torch.ones(C) #uniform weights for all classes\n",
    " \n",
    "\t\tdice = DiceLoss()\n",
    "\t\ttotalLoss = 0\n",
    " \n",
    "\t\tfor i in range(C):\n",
    "\t\t\tdiceLoss = dice(input[:,i], target[:,i])\n",
    "\t\t\ttotalLoss += diceLoss\n",
    " \n",
    "\t\treturn totalLoss\n",
    "\n",
    "def Dice(input, target):\n",
    "    N = target.size(0)\n",
    "    smooth = 1\n",
    "\n",
    "    input_flat = input.view(N, -1)\n",
    "    target_flat = target.view(N, -1)\n",
    "\n",
    "    intersection = input_flat * target_flat\n",
    "\n",
    "    loss = 2 * (intersection.sum(1) + smooth) / (input_flat.sum(1) + target_flat.sum(1) + smooth)\n",
    "    loss = loss.sum() / N\n",
    "\n",
    "    return loss\n",
    "\n",
    "def MutilDice(input, target):\n",
    "    C = target.shape[1]\n",
    " \n",
    "    # if weights is None:\n",
    "    # \tweights = torch.ones(C) #uniform weights for all classes\n",
    "\n",
    "    totalLoss = 0\n",
    "\n",
    "    for i in range(C):\n",
    "        diceLoss = Dice(input[:,i], target[:,i])\n",
    "        totalLoss += diceLoss\n",
    "\n",
    "    return totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim\n",
    "loss_func = MulticlassDiceLoss()\n",
    "#loss_func = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set,test_set = train_test_split(train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dice = 0\n",
    "best_epoch = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = MyDataset(train_set, transform = Transform, transform_label = Lable_transfrom)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "test_data = MyDataset(test_set, transform = Transform, transform_label = Lable_transfrom)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=16, shuffle=True)\n",
    "net = net.to('cuda:0')\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500\n",
      "loss: 2.6763 ; train dice : 0.3237 ; val dice: 0.0427; time: 11.3454s\n",
      "Best loss : 0.0427 at epoch 000\n",
      "Epoch: 2/500\n",
      "loss: 2.3721 ; train dice : 0.6279 ; val dice: 0.0848; time: 10.3367s\n",
      "Best loss : 0.0848 at epoch 001\n",
      "Epoch: 3/500\n",
      "loss: 2.3563 ; train dice : 0.6437 ; val dice: 0.5393; time: 10.4384s\n",
      "Best loss : 0.5393 at epoch 002\n",
      "Epoch: 4/500\n",
      "loss: 2.2924 ; train dice : 0.7076 ; val dice: 0.6817; time: 10.4331s\n",
      "Best loss : 0.6817 at epoch 003\n",
      "Epoch: 5/500\n",
      "loss: 2.2698 ; train dice : 0.7302 ; val dice: 0.6543; time: 10.3782s\n",
      "Best loss : 0.6817 at epoch 003\n",
      "Epoch: 6/500\n",
      "loss: 2.2419 ; train dice : 0.7581 ; val dice: 0.5445; time: 10.4248s\n",
      "Best loss : 0.6817 at epoch 003\n",
      "Epoch: 7/500\n",
      "loss: 2.2601 ; train dice : 0.7399 ; val dice: 0.6511; time: 10.4606s\n",
      "Best loss : 0.6817 at epoch 003\n",
      "Epoch: 8/500\n",
      "loss: 2.2067 ; train dice : 0.7933 ; val dice: 0.7494; time: 10.4616s\n",
      "Best loss : 0.7494 at epoch 007\n",
      "Epoch: 9/500\n",
      "loss: 2.1768 ; train dice : 0.8232 ; val dice: 0.7546; time: 10.4761s\n",
      "Best loss : 0.7546 at epoch 008\n",
      "Epoch: 10/500\n",
      "loss: 2.1713 ; train dice : 0.8287 ; val dice: 0.7606; time: 10.4992s\n",
      "Best loss : 0.7606 at epoch 009\n",
      "Epoch: 11/500\n",
      "loss: 2.1840 ; train dice : 0.8160 ; val dice: 0.7058; time: 10.4634s\n",
      "Best loss : 0.7606 at epoch 009\n",
      "Epoch: 12/500\n",
      "loss: 2.1796 ; train dice : 0.8204 ; val dice: 0.7174; time: 10.4497s\n",
      "Best loss : 0.7606 at epoch 009\n",
      "Epoch: 13/500\n",
      "loss: 2.1622 ; train dice : 0.8378 ; val dice: 0.7663; time: 10.5241s\n",
      "Best loss : 0.7663 at epoch 012\n",
      "Epoch: 14/500\n",
      "loss: 2.1265 ; train dice : 0.8735 ; val dice: 0.7997; time: 10.4982s\n",
      "Best loss : 0.7997 at epoch 013\n",
      "Epoch: 15/500\n",
      "loss: 2.1128 ; train dice : 0.8872 ; val dice: 0.7821; time: 10.3732s\n",
      "Best loss : 0.7997 at epoch 013\n",
      "Epoch: 16/500\n",
      "loss: 2.1152 ; train dice : 0.8848 ; val dice: 0.7400; time: 10.3734s\n",
      "Best loss : 0.7997 at epoch 013\n",
      "Epoch: 17/500\n",
      "loss: 2.1152 ; train dice : 0.8848 ; val dice: 0.7315; time: 10.4382s\n",
      "Best loss : 0.7997 at epoch 013\n",
      "Epoch: 18/500\n",
      "loss: 2.0824 ; train dice : 0.9176 ; val dice: 0.8396; time: 10.5024s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 19/500\n",
      "loss: 2.0915 ; train dice : 0.9085 ; val dice: 0.8359; time: 10.4627s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 20/500\n",
      "loss: 2.0877 ; train dice : 0.9123 ; val dice: 0.7313; time: 10.4479s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 21/500\n",
      "loss: 2.0677 ; train dice : 0.9323 ; val dice: 0.8296; time: 10.4586s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 22/500\n",
      "loss: 2.0418 ; train dice : 0.9582 ; val dice: 0.8351; time: 10.4555s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 23/500\n",
      "loss: 2.0371 ; train dice : 0.9629 ; val dice: 0.8390; time: 10.4558s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 24/500\n",
      "loss: 2.0427 ; train dice : 0.9573 ; val dice: 0.8173; time: 10.4520s\n",
      "Best loss : 0.8396 at epoch 017\n",
      "Epoch: 25/500\n",
      "loss: 2.0418 ; train dice : 0.9582 ; val dice: 0.8496; time: 10.5200s\n",
      "Best loss : 0.8496 at epoch 024\n",
      "Epoch: 26/500\n",
      "loss: 2.0221 ; train dice : 0.9779 ; val dice: 0.8902; time: 10.5355s\n",
      "Best loss : 0.8902 at epoch 025\n",
      "Epoch: 27/500\n",
      "loss: 1.9967 ; train dice : 1.0033 ; val dice: 0.8936; time: 10.5144s\n",
      "Best loss : 0.8936 at epoch 026\n",
      "Epoch: 28/500\n",
      "loss: 1.9952 ; train dice : 1.0048 ; val dice: 0.8877; time: 10.4662s\n",
      "Best loss : 0.8936 at epoch 026\n",
      "Epoch: 29/500\n",
      "loss: 1.9728 ; train dice : 1.0272 ; val dice: 0.9009; time: 10.5921s\n",
      "Best loss : 0.9009 at epoch 028\n",
      "Epoch: 30/500\n",
      "loss: 1.9884 ; train dice : 1.0116 ; val dice: 0.8255; time: 10.4678s\n",
      "Best loss : 0.9009 at epoch 028\n",
      "Epoch: 31/500\n",
      "loss: 1.9949 ; train dice : 1.0051 ; val dice: 0.9137; time: 10.5476s\n",
      "Best loss : 0.9137 at epoch 030\n",
      "Epoch: 32/500\n",
      "loss: 1.9583 ; train dice : 1.0417 ; val dice: 0.9391; time: 10.5354s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 33/500\n",
      "loss: 1.9618 ; train dice : 1.0382 ; val dice: 0.8997; time: 10.4861s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 34/500\n",
      "loss: 1.9591 ; train dice : 1.0409 ; val dice: 0.9322; time: 10.4741s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 35/500\n",
      "loss: 1.9526 ; train dice : 1.0474 ; val dice: 0.9284; time: 10.4585s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 36/500\n",
      "loss: 1.9727 ; train dice : 1.0273 ; val dice: 0.8675; time: 10.4888s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 37/500\n",
      "loss: 1.9418 ; train dice : 1.0582 ; val dice: 0.9015; time: 10.4802s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 38/500\n",
      "loss: 1.9478 ; train dice : 1.0522 ; val dice: 0.7069; time: 10.4758s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 39/500\n",
      "loss: 1.9389 ; train dice : 1.0611 ; val dice: 0.9180; time: 10.4475s\n",
      "Best loss : 0.9391 at epoch 031\n",
      "Epoch: 40/500\n",
      "loss: 1.9078 ; train dice : 1.0922 ; val dice: 0.9445; time: 10.5106s\n",
      "Best loss : 0.9445 at epoch 039\n",
      "Epoch: 41/500\n",
      "loss: 1.9075 ; train dice : 1.0925 ; val dice: 0.9625; time: 10.5188s\n",
      "Best loss : 0.9625 at epoch 040\n",
      "Epoch: 42/500\n",
      "loss: 1.8958 ; train dice : 1.1042 ; val dice: 0.9304; time: 10.4914s\n",
      "Best loss : 0.9625 at epoch 040\n",
      "Epoch: 43/500\n",
      "loss: 1.9033 ; train dice : 1.0967 ; val dice: 0.9532; time: 10.4705s\n",
      "Best loss : 0.9625 at epoch 040\n",
      "Epoch: 44/500\n",
      "loss: 1.9468 ; train dice : 1.0532 ; val dice: 0.9040; time: 10.4611s\n",
      "Best loss : 0.9625 at epoch 040\n",
      "Epoch: 45/500\n",
      "loss: 1.9035 ; train dice : 1.0965 ; val dice: 0.9769; time: 10.5303s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 46/500\n",
      "loss: 1.9084 ; train dice : 1.0916 ; val dice: 0.8740; time: 10.4883s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 47/500\n",
      "loss: 1.8955 ; train dice : 1.1045 ; val dice: 0.9718; time: 10.4953s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 48/500\n",
      "loss: 1.9066 ; train dice : 1.0934 ; val dice: 0.9613; time: 10.4555s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 49/500\n",
      "loss: 1.9067 ; train dice : 1.0933 ; val dice: 0.9310; time: 10.4946s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 50/500\n",
      "loss: 1.8792 ; train dice : 1.1208 ; val dice: 0.9032; time: 10.4653s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 51/500\n",
      "loss: 1.8773 ; train dice : 1.1227 ; val dice: 0.7372; time: 10.5017s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 52/500\n",
      "loss: 1.8537 ; train dice : 1.1463 ; val dice: 0.9641; time: 10.4626s\n",
      "Best loss : 0.9769 at epoch 044\n",
      "Epoch: 53/500\n",
      "loss: 1.8554 ; train dice : 1.1446 ; val dice: 0.9902; time: 10.5531s\n",
      "Best loss : 0.9902 at epoch 052\n",
      "Epoch: 54/500\n",
      "loss: 1.8580 ; train dice : 1.1420 ; val dice: 0.9692; time: 10.4752s\n",
      "Best loss : 0.9902 at epoch 052\n",
      "Epoch: 55/500\n",
      "loss: 1.8518 ; train dice : 1.1482 ; val dice: 0.9717; time: 10.4990s\n",
      "Best loss : 0.9902 at epoch 052\n",
      "Epoch: 56/500\n",
      "loss: 1.8234 ; train dice : 1.1766 ; val dice: 0.9991; time: 10.5359s\n",
      "Best loss : 0.9991 at epoch 055\n",
      "Epoch: 57/500\n",
      "loss: 1.8293 ; train dice : 1.1707 ; val dice: 0.9802; time: 10.5003s\n",
      "Best loss : 0.9991 at epoch 055\n",
      "Epoch: 58/500\n",
      "loss: 1.8493 ; train dice : 1.1507 ; val dice: 0.9841; time: 10.4622s\n",
      "Best loss : 0.9991 at epoch 055\n",
      "Epoch: 59/500\n",
      "loss: 1.8317 ; train dice : 1.1683 ; val dice: 0.9899; time: 10.5101s\n",
      "Best loss : 0.9991 at epoch 055\n",
      "Epoch: 60/500\n",
      "loss: 1.8174 ; train dice : 1.1826 ; val dice: 0.9208; time: 10.4512s\n",
      "Best loss : 0.9991 at epoch 055\n",
      "Epoch: 61/500\n",
      "loss: 1.7994 ; train dice : 1.2006 ; val dice: 1.0250; time: 10.5273s\n",
      "Best loss : 1.0250 at epoch 060\n",
      "Epoch: 62/500\n",
      "loss: 1.8076 ; train dice : 1.1924 ; val dice: 0.9866; time: 10.4900s\n",
      "Best loss : 1.0250 at epoch 060\n",
      "Epoch: 63/500\n",
      "loss: 1.8642 ; train dice : 1.1358 ; val dice: 0.9833; time: 10.4740s\n",
      "Best loss : 1.0250 at epoch 060\n",
      "Epoch: 64/500\n",
      "loss: 1.8366 ; train dice : 1.1634 ; val dice: 1.0097; time: 10.4589s\n",
      "Best loss : 1.0250 at epoch 060\n",
      "Epoch: 65/500\n",
      "loss: 1.8142 ; train dice : 1.1858 ; val dice: 1.0098; time: 10.4494s\n",
      "Best loss : 1.0250 at epoch 060\n",
      "Epoch: 66/500\n",
      "loss: 1.7812 ; train dice : 1.2188 ; val dice: 1.0421; time: 10.5102s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 67/500\n",
      "loss: 1.7935 ; train dice : 1.2065 ; val dice: 1.0393; time: 10.4650s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 68/500\n",
      "loss: 1.8105 ; train dice : 1.1895 ; val dice: 1.0414; time: 10.4593s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 69/500\n",
      "loss: 1.7757 ; train dice : 1.2243 ; val dice: 1.0157; time: 10.4836s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 70/500\n",
      "loss: 1.7811 ; train dice : 1.2189 ; val dice: 1.0367; time: 10.4425s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 71/500\n",
      "loss: 1.7797 ; train dice : 1.2203 ; val dice: 1.0384; time: 10.4655s\n",
      "Best loss : 1.0421 at epoch 065\n",
      "Epoch: 72/500\n",
      "loss: 1.7744 ; train dice : 1.2256 ; val dice: 1.0576; time: 10.4937s\n",
      "Best loss : 1.0576 at epoch 071\n",
      "Epoch: 73/500\n",
      "loss: 1.7722 ; train dice : 1.2278 ; val dice: 1.0410; time: 10.4794s\n",
      "Best loss : 1.0576 at epoch 071\n",
      "Epoch: 74/500\n",
      "loss: 1.7478 ; train dice : 1.2522 ; val dice: 1.0867; time: 10.5291s\n",
      "Best loss : 1.0867 at epoch 073\n",
      "Epoch: 75/500\n",
      "loss: 1.7239 ; train dice : 1.2761 ; val dice: 1.0683; time: 10.4911s\n",
      "Best loss : 1.0867 at epoch 073\n",
      "Epoch: 76/500\n",
      "loss: 1.7221 ; train dice : 1.2779 ; val dice: 1.1048; time: 10.5299s\n",
      "Best loss : 1.1048 at epoch 075\n",
      "Epoch: 77/500\n",
      "loss: 1.7395 ; train dice : 1.2605 ; val dice: 0.9731; time: 10.4937s\n",
      "Best loss : 1.1048 at epoch 075\n",
      "Epoch: 78/500\n",
      "loss: 1.7658 ; train dice : 1.2342 ; val dice: 1.0730; time: 10.4804s\n",
      "Best loss : 1.1048 at epoch 075\n",
      "Epoch: 79/500\n",
      "loss: 1.7365 ; train dice : 1.2635 ; val dice: 1.1061; time: 10.5071s\n",
      "Best loss : 1.1061 at epoch 078\n",
      "Epoch: 80/500\n",
      "loss: 1.7366 ; train dice : 1.2634 ; val dice: 1.1080; time: 10.5100s\n",
      "Best loss : 1.1080 at epoch 079\n",
      "Epoch: 81/500\n",
      "loss: 1.7261 ; train dice : 1.2739 ; val dice: 1.0944; time: 10.4742s\n",
      "Best loss : 1.1080 at epoch 079\n",
      "Epoch: 82/500\n",
      "loss: 1.7166 ; train dice : 1.2834 ; val dice: 1.0770; time: 10.5069s\n",
      "Best loss : 1.1080 at epoch 079\n",
      "Epoch: 83/500\n",
      "loss: 1.7325 ; train dice : 1.2675 ; val dice: 1.0848; time: 10.4784s\n",
      "Best loss : 1.1080 at epoch 079\n",
      "Epoch: 84/500\n",
      "loss: 1.7100 ; train dice : 1.2900 ; val dice: 1.0425; time: 10.5181s\n",
      "Best loss : 1.1080 at epoch 079\n",
      "Epoch: 85/500\n",
      "loss: 1.7355 ; train dice : 1.2645 ; val dice: 1.1119; time: 10.5487s\n",
      "Best loss : 1.1119 at epoch 084\n",
      "Epoch: 86/500\n",
      "loss: 1.7167 ; train dice : 1.2833 ; val dice: 1.0980; time: 10.4997s\n",
      "Best loss : 1.1119 at epoch 084\n",
      "Epoch: 87/500\n",
      "loss: 1.6985 ; train dice : 1.3015 ; val dice: 1.1185; time: 10.5144s\n",
      "Best loss : 1.1185 at epoch 086\n",
      "Epoch: 88/500\n",
      "loss: 1.6933 ; train dice : 1.3067 ; val dice: 1.1247; time: 10.5048s\n",
      "Best loss : 1.1247 at epoch 087\n",
      "Epoch: 89/500\n",
      "loss: 1.6881 ; train dice : 1.3119 ; val dice: 1.1112; time: 10.4798s\n",
      "Best loss : 1.1247 at epoch 087\n",
      "Epoch: 90/500\n",
      "loss: 1.6907 ; train dice : 1.3093 ; val dice: 1.1237; time: 10.4752s\n",
      "Best loss : 1.1247 at epoch 087\n",
      "Epoch: 91/500\n",
      "loss: 1.6915 ; train dice : 1.3085 ; val dice: 1.1109; time: 10.4664s\n",
      "Best loss : 1.1247 at epoch 087\n",
      "Epoch: 92/500\n",
      "loss: 1.6971 ; train dice : 1.3029 ; val dice: 1.1286; time: 10.5542s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 93/500\n",
      "loss: 1.6961 ; train dice : 1.3039 ; val dice: 1.1110; time: 10.5159s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 94/500\n",
      "loss: 1.7014 ; train dice : 1.2986 ; val dice: 1.0723; time: 10.4333s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 95/500\n",
      "loss: 1.7008 ; train dice : 1.2992 ; val dice: 1.0863; time: 10.4805s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 96/500\n",
      "loss: 1.6898 ; train dice : 1.3102 ; val dice: 1.0786; time: 10.5132s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 97/500\n",
      "loss: 1.6914 ; train dice : 1.3086 ; val dice: 1.1250; time: 10.4496s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 98/500\n",
      "loss: 1.6932 ; train dice : 1.3068 ; val dice: 1.0759; time: 10.4598s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 99/500\n",
      "loss: 1.6819 ; train dice : 1.3181 ; val dice: 1.1000; time: 10.4617s\n",
      "Best loss : 1.1286 at epoch 091\n",
      "Epoch: 100/500\n",
      "loss: 1.6655 ; train dice : 1.3345 ; val dice: 1.1392; time: 10.4986s\n",
      "Best loss : 1.1392 at epoch 099\n",
      "Epoch: 101/500\n",
      "loss: 1.6556 ; train dice : 1.3444 ; val dice: 1.1408; time: 10.4903s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 102/500\n",
      "loss: 1.6934 ; train dice : 1.3066 ; val dice: 1.0758; time: 10.4832s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 103/500\n",
      "loss: 1.6800 ; train dice : 1.3200 ; val dice: 1.1048; time: 10.4689s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 104/500\n",
      "loss: 1.6671 ; train dice : 1.3329 ; val dice: 1.1080; time: 10.4544s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 105/500\n",
      "loss: 1.6560 ; train dice : 1.3440 ; val dice: 1.1275; time: 10.4554s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 106/500\n",
      "loss: 1.6663 ; train dice : 1.3337 ; val dice: 1.1066; time: 10.4646s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 107/500\n",
      "loss: 1.6885 ; train dice : 1.3115 ; val dice: 1.1035; time: 10.4589s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 108/500\n",
      "loss: 1.6694 ; train dice : 1.3306 ; val dice: 1.1061; time: 10.4716s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 109/500\n",
      "loss: 1.6703 ; train dice : 1.3297 ; val dice: 1.1290; time: 10.4442s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 110/500\n",
      "loss: 1.6692 ; train dice : 1.3308 ; val dice: 1.1094; time: 10.4547s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 111/500\n",
      "loss: 1.6573 ; train dice : 1.3427 ; val dice: 1.1396; time: 10.4549s\n",
      "Best loss : 1.1408 at epoch 100\n",
      "Epoch: 112/500\n",
      "loss: 1.6421 ; train dice : 1.3579 ; val dice: 1.1521; time: 10.5342s\n",
      "Best loss : 1.1521 at epoch 111\n",
      "Epoch: 113/500\n",
      "loss: 1.6392 ; train dice : 1.3608 ; val dice: 1.1607; time: 10.4867s\n",
      "Best loss : 1.1607 at epoch 112\n",
      "Epoch: 114/500\n",
      "loss: 1.6332 ; train dice : 1.3668 ; val dice: 1.1363; time: 10.4501s\n",
      "Best loss : 1.1607 at epoch 112\n",
      "Epoch: 115/500\n",
      "loss: 1.6313 ; train dice : 1.3687 ; val dice: 1.1672; time: 10.5161s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 116/500\n",
      "loss: 1.6370 ; train dice : 1.3630 ; val dice: 1.1288; time: 10.4703s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 117/500\n",
      "loss: 1.6423 ; train dice : 1.3577 ; val dice: 1.1236; time: 10.4602s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 118/500\n",
      "loss: 1.6484 ; train dice : 1.3516 ; val dice: 1.1167; time: 10.4415s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 119/500\n",
      "loss: 1.6494 ; train dice : 1.3506 ; val dice: 1.1409; time: 10.5114s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 120/500\n",
      "loss: 1.6402 ; train dice : 1.3598 ; val dice: 1.1271; time: 10.5066s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 121/500\n",
      "loss: 1.6785 ; train dice : 1.3215 ; val dice: 1.0887; time: 10.4400s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 122/500\n",
      "loss: 1.7269 ; train dice : 1.2731 ; val dice: 1.0202; time: 10.4523s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 123/500\n",
      "loss: 1.6761 ; train dice : 1.3239 ; val dice: 1.0921; time: 10.4651s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 124/500\n",
      "loss: 1.6386 ; train dice : 1.3614 ; val dice: 1.1553; time: 10.4602s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 125/500\n",
      "loss: 1.6300 ; train dice : 1.3700 ; val dice: 1.1591; time: 10.4827s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 126/500\n",
      "loss: 1.6183 ; train dice : 1.3817 ; val dice: 1.1402; time: 10.4909s\n",
      "Best loss : 1.1672 at epoch 114\n",
      "Epoch: 127/500\n",
      "loss: 1.6171 ; train dice : 1.3829 ; val dice: 1.1708; time: 10.5209s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 128/500\n",
      "loss: 1.6129 ; train dice : 1.3871 ; val dice: 1.1672; time: 10.4417s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 129/500\n",
      "loss: 1.6154 ; train dice : 1.3846 ; val dice: 1.1573; time: 10.4660s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 130/500\n",
      "loss: 1.6141 ; train dice : 1.3859 ; val dice: 1.1487; time: 10.4866s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 131/500\n",
      "loss: 1.6124 ; train dice : 1.3876 ; val dice: 1.1544; time: 10.4947s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 132/500\n",
      "loss: 1.6223 ; train dice : 1.3777 ; val dice: 1.1340; time: 10.5041s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 133/500\n",
      "loss: 1.6185 ; train dice : 1.3815 ; val dice: 1.1311; time: 10.4713s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 134/500\n",
      "loss: 1.6357 ; train dice : 1.3643 ; val dice: 1.1064; time: 10.4645s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 135/500\n",
      "loss: 1.6519 ; train dice : 1.3481 ; val dice: 1.1355; time: 10.4407s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 136/500\n",
      "loss: 1.6336 ; train dice : 1.3664 ; val dice: 1.1095; time: 10.4603s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 137/500\n",
      "loss: 1.6194 ; train dice : 1.3806 ; val dice: 1.1644; time: 10.4529s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 138/500\n",
      "loss: 1.6075 ; train dice : 1.3925 ; val dice: 1.1558; time: 10.4541s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 139/500\n",
      "loss: 1.6067 ; train dice : 1.3933 ; val dice: 1.1585; time: 10.4573s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 140/500\n",
      "loss: 1.6037 ; train dice : 1.3963 ; val dice: 1.1665; time: 10.4913s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 141/500\n",
      "loss: 1.5981 ; train dice : 1.4019 ; val dice: 1.1578; time: 10.4799s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 142/500\n",
      "loss: 1.6027 ; train dice : 1.3973 ; val dice: 1.1553; time: 10.4651s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 143/500\n",
      "loss: 1.6242 ; train dice : 1.3758 ; val dice: 1.1442; time: 10.4751s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 144/500\n",
      "loss: 1.6221 ; train dice : 1.3779 ; val dice: 1.1421; time: 10.4419s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 145/500\n",
      "loss: 1.6129 ; train dice : 1.3871 ; val dice: 1.1441; time: 10.4448s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 146/500\n",
      "loss: 1.6225 ; train dice : 1.3775 ; val dice: 1.1296; time: 10.4645s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 147/500\n",
      "loss: 1.6179 ; train dice : 1.3821 ; val dice: 1.1676; time: 10.4678s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 148/500\n",
      "loss: 1.6100 ; train dice : 1.3900 ; val dice: 1.1439; time: 10.4699s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 149/500\n",
      "loss: 1.5997 ; train dice : 1.4003 ; val dice: 1.1538; time: 10.4573s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 150/500\n",
      "loss: 1.5986 ; train dice : 1.4014 ; val dice: 1.1608; time: 10.4966s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 151/500\n",
      "loss: 1.6066 ; train dice : 1.3934 ; val dice: 1.0837; time: 10.4381s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 152/500\n",
      "loss: 1.6201 ; train dice : 1.3799 ; val dice: 1.1434; time: 10.4619s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 153/500\n",
      "loss: 1.6052 ; train dice : 1.3948 ; val dice: 1.1648; time: 10.4608s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 154/500\n",
      "loss: 1.5965 ; train dice : 1.4035 ; val dice: 1.1606; time: 10.4490s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 155/500\n",
      "loss: 1.5950 ; train dice : 1.4050 ; val dice: 1.1602; time: 10.4377s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 156/500\n",
      "loss: 1.5995 ; train dice : 1.4005 ; val dice: 1.1400; time: 10.4685s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 157/500\n",
      "loss: 1.5996 ; train dice : 1.4004 ; val dice: 1.1461; time: 10.4512s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 158/500\n",
      "loss: 1.6022 ; train dice : 1.3978 ; val dice: 1.1512; time: 10.4429s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 159/500\n",
      "loss: 1.5990 ; train dice : 1.4010 ; val dice: 1.1518; time: 10.4588s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 160/500\n",
      "loss: 1.5935 ; train dice : 1.4065 ; val dice: 1.1530; time: 10.4939s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 161/500\n",
      "loss: 1.5932 ; train dice : 1.4068 ; val dice: 1.1539; time: 10.4447s\n",
      "Best loss : 1.1708 at epoch 126\n",
      "Epoch: 162/500\n",
      "loss: 1.5859 ; train dice : 1.4141 ; val dice: 1.1838; time: 10.5226s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 163/500\n",
      "loss: 1.5834 ; train dice : 1.4166 ; val dice: 1.1750; time: 10.4563s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 164/500\n",
      "loss: 1.5915 ; train dice : 1.4085 ; val dice: 1.1151; time: 10.4923s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 165/500\n",
      "loss: 1.5964 ; train dice : 1.4036 ; val dice: 1.1554; time: 10.4852s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 166/500\n",
      "loss: 1.5892 ; train dice : 1.4108 ; val dice: 1.1657; time: 10.4897s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 167/500\n",
      "loss: 1.5873 ; train dice : 1.4127 ; val dice: 1.1675; time: 10.4650s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 168/500\n",
      "loss: 1.6045 ; train dice : 1.3955 ; val dice: 1.0872; time: 10.4719s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 169/500\n",
      "loss: 1.6278 ; train dice : 1.3722 ; val dice: 1.1349; time: 10.4521s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 170/500\n",
      "loss: 1.6033 ; train dice : 1.3967 ; val dice: 1.1712; time: 10.4879s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 171/500\n",
      "loss: 1.6038 ; train dice : 1.3962 ; val dice: 1.1343; time: 10.4740s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 172/500\n",
      "loss: 1.5892 ; train dice : 1.4108 ; val dice: 1.1759; time: 10.4713s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 173/500\n",
      "loss: 1.5820 ; train dice : 1.4180 ; val dice: 1.1746; time: 10.4824s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 174/500\n",
      "loss: 1.5759 ; train dice : 1.4241 ; val dice: 1.1681; time: 10.4636s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 175/500\n",
      "loss: 1.5744 ; train dice : 1.4256 ; val dice: 1.1644; time: 10.4707s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 176/500\n",
      "loss: 1.5763 ; train dice : 1.4237 ; val dice: 1.1364; time: 10.4841s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 177/500\n",
      "loss: 1.5912 ; train dice : 1.4088 ; val dice: 1.1516; time: 10.4925s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 178/500\n",
      "loss: 1.6048 ; train dice : 1.3952 ; val dice: 1.0853; time: 10.4535s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 179/500\n",
      "loss: 1.6403 ; train dice : 1.3597 ; val dice: 1.0987; time: 10.4622s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 180/500\n",
      "loss: 1.6054 ; train dice : 1.3946 ; val dice: 1.1577; time: 10.4829s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 181/500\n",
      "loss: 1.6063 ; train dice : 1.3937 ; val dice: 1.1330; time: 10.3890s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 182/500\n",
      "loss: 1.5857 ; train dice : 1.4143 ; val dice: 1.1519; time: 10.3629s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 183/500\n",
      "loss: 1.5759 ; train dice : 1.4241 ; val dice: 1.1764; time: 10.3725s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 184/500\n",
      "loss: 1.5748 ; train dice : 1.4252 ; val dice: 1.1592; time: 10.3760s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 185/500\n",
      "loss: 1.5767 ; train dice : 1.4233 ; val dice: 1.1740; time: 10.3776s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 186/500\n",
      "loss: 1.5734 ; train dice : 1.4266 ; val dice: 1.1688; time: 10.3711s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 187/500\n",
      "loss: 1.5691 ; train dice : 1.4309 ; val dice: 1.1730; time: 10.3707s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 188/500\n",
      "loss: 1.5668 ; train dice : 1.4332 ; val dice: 1.1729; time: 10.3695s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 189/500\n",
      "loss: 1.5719 ; train dice : 1.4281 ; val dice: 1.1625; time: 10.3747s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 190/500\n",
      "loss: 1.5761 ; train dice : 1.4239 ; val dice: 1.1649; time: 10.3752s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 191/500\n",
      "loss: 1.5816 ; train dice : 1.4184 ; val dice: 1.1672; time: 10.3795s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 192/500\n",
      "loss: 1.5757 ; train dice : 1.4243 ; val dice: 1.1541; time: 10.3717s\n",
      "Best loss : 1.1838 at epoch 161\n",
      "Epoch: 193/500\n",
      "loss: 1.5767 ; train dice : 1.4233 ; val dice: 1.1898; time: 10.4522s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 194/500\n",
      "loss: 1.5741 ; train dice : 1.4259 ; val dice: 1.1400; time: 10.4799s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 195/500\n",
      "loss: 1.6258 ; train dice : 1.3742 ; val dice: 0.9746; time: 10.4737s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 196/500\n",
      "loss: 1.6010 ; train dice : 1.3990 ; val dice: 1.1803; time: 10.4733s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 197/500\n",
      "loss: 1.5812 ; train dice : 1.4188 ; val dice: 1.1537; time: 10.4674s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 198/500\n",
      "loss: 1.5731 ; train dice : 1.4269 ; val dice: 1.1413; time: 10.4723s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 199/500\n",
      "loss: 1.5703 ; train dice : 1.4297 ; val dice: 1.1648; time: 10.4886s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 200/500\n",
      "loss: 1.5658 ; train dice : 1.4342 ; val dice: 1.1738; time: 10.4838s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 201/500\n",
      "loss: 1.5665 ; train dice : 1.4335 ; val dice: 1.1692; time: 10.4759s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 202/500\n",
      "loss: 1.5659 ; train dice : 1.4341 ; val dice: 1.1597; time: 10.4616s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 203/500\n",
      "loss: 1.5742 ; train dice : 1.4258 ; val dice: 1.1390; time: 10.4691s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 204/500\n",
      "loss: 1.5680 ; train dice : 1.4320 ; val dice: 1.1320; time: 10.4438s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 205/500\n",
      "loss: 1.5702 ; train dice : 1.4298 ; val dice: 1.1828; time: 10.4529s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 206/500\n",
      "loss: 1.5732 ; train dice : 1.4268 ; val dice: 1.1509; time: 10.4439s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 207/500\n",
      "loss: 1.5737 ; train dice : 1.4263 ; val dice: 1.1514; time: 10.4681s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 208/500\n",
      "loss: 1.5757 ; train dice : 1.4243 ; val dice: 1.1541; time: 10.4647s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 209/500\n",
      "loss: 1.5856 ; train dice : 1.4144 ; val dice: 1.1302; time: 10.4297s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 210/500\n",
      "loss: 1.5746 ; train dice : 1.4254 ; val dice: 1.1598; time: 10.4609s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 211/500\n",
      "loss: 1.5670 ; train dice : 1.4330 ; val dice: 1.1671; time: 10.4778s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 212/500\n",
      "loss: 1.5635 ; train dice : 1.4365 ; val dice: 1.1630; time: 10.4508s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 213/500\n",
      "loss: 1.5623 ; train dice : 1.4377 ; val dice: 1.1857; time: 10.4469s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 214/500\n",
      "loss: 1.5613 ; train dice : 1.4387 ; val dice: 1.1691; time: 10.4381s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 215/500\n",
      "loss: 1.5603 ; train dice : 1.4397 ; val dice: 1.1785; time: 10.4519s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 216/500\n",
      "loss: 1.5621 ; train dice : 1.4379 ; val dice: 1.1800; time: 10.4518s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 217/500\n",
      "loss: 1.5629 ; train dice : 1.4371 ; val dice: 1.1553; time: 10.4522s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 218/500\n",
      "loss: 1.5656 ; train dice : 1.4344 ; val dice: 1.1769; time: 10.4520s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 219/500\n",
      "loss: 1.5696 ; train dice : 1.4304 ; val dice: 1.1463; time: 10.4539s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 220/500\n",
      "loss: 1.5741 ; train dice : 1.4259 ; val dice: 1.1558; time: 10.4382s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 221/500\n",
      "loss: 1.5719 ; train dice : 1.4281 ; val dice: 1.1335; time: 10.4428s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 222/500\n",
      "loss: 1.5750 ; train dice : 1.4250 ; val dice: 1.1217; time: 10.4635s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 223/500\n",
      "loss: 1.5747 ; train dice : 1.4253 ; val dice: 1.1708; time: 10.4721s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 224/500\n",
      "loss: 1.5812 ; train dice : 1.4188 ; val dice: 1.1332; time: 10.4580s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 225/500\n",
      "loss: 1.5907 ; train dice : 1.4093 ; val dice: 1.1432; time: 10.4439s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 226/500\n",
      "loss: 1.5762 ; train dice : 1.4238 ; val dice: 1.1771; time: 10.4556s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 227/500\n",
      "loss: 1.5607 ; train dice : 1.4393 ; val dice: 1.1779; time: 10.4978s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 228/500\n",
      "loss: 1.5562 ; train dice : 1.4438 ; val dice: 1.1874; time: 10.5036s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 229/500\n",
      "loss: 1.5614 ; train dice : 1.4386 ; val dice: 1.1552; time: 10.4521s\n",
      "Best loss : 1.1898 at epoch 192\n",
      "Epoch: 230/500\n",
      "loss: 1.5605 ; train dice : 1.4395 ; val dice: 1.1980; time: 10.5453s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 231/500\n",
      "loss: 1.5571 ; train dice : 1.4429 ; val dice: 1.1898; time: 10.4871s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 232/500\n",
      "loss: 1.5561 ; train dice : 1.4439 ; val dice: 1.1885; time: 10.5079s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 233/500\n",
      "loss: 1.5524 ; train dice : 1.4476 ; val dice: 1.1900; time: 10.4609s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 234/500\n",
      "loss: 1.5527 ; train dice : 1.4473 ; val dice: 1.1865; time: 10.4491s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 235/500\n",
      "loss: 1.5515 ; train dice : 1.4485 ; val dice: 1.1803; time: 10.4459s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 236/500\n",
      "loss: 1.5523 ; train dice : 1.4477 ; val dice: 1.1659; time: 10.4537s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 237/500\n",
      "loss: 1.5528 ; train dice : 1.4472 ; val dice: 1.1693; time: 10.4452s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 238/500\n",
      "loss: 1.5569 ; train dice : 1.4431 ; val dice: 1.1634; time: 10.4369s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 239/500\n",
      "loss: 1.5600 ; train dice : 1.4400 ; val dice: 1.1727; time: 10.4399s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 240/500\n",
      "loss: 1.5606 ; train dice : 1.4394 ; val dice: 1.1634; time: 10.4382s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 241/500\n",
      "loss: 1.5622 ; train dice : 1.4378 ; val dice: 1.1771; time: 10.4770s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 242/500\n",
      "loss: 1.5592 ; train dice : 1.4408 ; val dice: 1.1584; time: 10.4575s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 243/500\n",
      "loss: 1.5598 ; train dice : 1.4402 ; val dice: 1.1439; time: 10.4978s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 244/500\n",
      "loss: 1.5593 ; train dice : 1.4407 ; val dice: 1.1792; time: 10.4899s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 245/500\n",
      "loss: 1.5547 ; train dice : 1.4453 ; val dice: 1.1774; time: 10.4715s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 246/500\n",
      "loss: 1.5600 ; train dice : 1.4400 ; val dice: 1.1661; time: 10.5045s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 247/500\n",
      "loss: 1.5660 ; train dice : 1.4340 ; val dice: 1.1496; time: 10.4965s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 248/500\n",
      "loss: 1.5627 ; train dice : 1.4373 ; val dice: 1.1583; time: 10.5159s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 249/500\n",
      "loss: 1.5616 ; train dice : 1.4384 ; val dice: 1.1649; time: 10.5173s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 250/500\n",
      "loss: 1.5557 ; train dice : 1.4443 ; val dice: 1.1743; time: 10.4519s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 251/500\n",
      "loss: 1.5514 ; train dice : 1.4486 ; val dice: 1.1418; time: 10.4380s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 252/500\n",
      "loss: 1.5534 ; train dice : 1.4466 ; val dice: 1.1741; time: 10.4379s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 253/500\n",
      "loss: 1.5500 ; train dice : 1.4500 ; val dice: 1.1665; time: 10.4569s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 254/500\n",
      "loss: 1.5519 ; train dice : 1.4481 ; val dice: 1.1704; time: 10.4461s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 255/500\n",
      "loss: 1.5596 ; train dice : 1.4404 ; val dice: 1.1619; time: 10.4577s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 256/500\n",
      "loss: 1.5561 ; train dice : 1.4439 ; val dice: 1.1641; time: 10.4682s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 257/500\n",
      "loss: 1.5571 ; train dice : 1.4429 ; val dice: 1.1608; time: 10.4599s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 258/500\n",
      "loss: 1.5618 ; train dice : 1.4382 ; val dice: 1.1793; time: 10.4564s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 259/500\n",
      "loss: 1.5548 ; train dice : 1.4452 ; val dice: 1.1560; time: 10.4640s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 260/500\n",
      "loss: 1.5602 ; train dice : 1.4398 ; val dice: 1.1289; time: 10.4445s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 261/500\n",
      "loss: 1.5603 ; train dice : 1.4397 ; val dice: 1.1351; time: 10.4596s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 262/500\n",
      "loss: 1.5617 ; train dice : 1.4383 ; val dice: 1.1547; time: 10.4614s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 263/500\n",
      "loss: 1.5522 ; train dice : 1.4478 ; val dice: 1.1529; time: 10.4384s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 264/500\n",
      "loss: 1.5542 ; train dice : 1.4458 ; val dice: 1.1483; time: 10.4959s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 265/500\n",
      "loss: 1.5498 ; train dice : 1.4502 ; val dice: 1.1474; time: 10.4793s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 266/500\n",
      "loss: 1.5551 ; train dice : 1.4449 ; val dice: 1.1693; time: 10.4321s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 267/500\n",
      "loss: 1.5551 ; train dice : 1.4449 ; val dice: 1.1644; time: 10.4612s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 268/500\n",
      "loss: 1.5591 ; train dice : 1.4409 ; val dice: 1.1430; time: 10.4486s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 269/500\n",
      "loss: 1.5595 ; train dice : 1.4405 ; val dice: 1.0155; time: 10.4697s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 270/500\n",
      "loss: 1.5568 ; train dice : 1.4432 ; val dice: 1.1699; time: 10.4459s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 271/500\n",
      "loss: 1.5548 ; train dice : 1.4452 ; val dice: 1.1688; time: 10.4509s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 272/500\n",
      "loss: 1.5477 ; train dice : 1.4523 ; val dice: 1.1633; time: 10.4417s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 273/500\n",
      "loss: 1.5462 ; train dice : 1.4538 ; val dice: 1.1802; time: 10.4361s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 274/500\n",
      "loss: 1.5483 ; train dice : 1.4517 ; val dice: 1.1690; time: 10.4440s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 275/500\n",
      "loss: 1.5454 ; train dice : 1.4546 ; val dice: 1.1750; time: 10.5103s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 276/500\n",
      "loss: 1.5455 ; train dice : 1.4545 ; val dice: 1.1737; time: 10.4947s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 277/500\n",
      "loss: 1.5497 ; train dice : 1.4503 ; val dice: 1.1555; time: 10.4812s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 278/500\n",
      "loss: 1.5541 ; train dice : 1.4459 ; val dice: 1.1751; time: 10.4819s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 279/500\n",
      "loss: 1.5497 ; train dice : 1.4503 ; val dice: 1.1684; time: 10.4487s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 280/500\n",
      "loss: 1.5483 ; train dice : 1.4517 ; val dice: 1.1425; time: 10.4445s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 281/500\n",
      "loss: 1.5484 ; train dice : 1.4516 ; val dice: 1.1650; time: 10.4478s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 282/500\n",
      "loss: 1.5489 ; train dice : 1.4511 ; val dice: 1.1657; time: 10.4601s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 283/500\n",
      "loss: 1.5497 ; train dice : 1.4503 ; val dice: 1.1573; time: 10.4872s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 284/500\n",
      "loss: 1.5502 ; train dice : 1.4498 ; val dice: 1.1769; time: 10.4702s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 285/500\n",
      "loss: 1.5525 ; train dice : 1.4475 ; val dice: 1.1516; time: 10.4514s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 286/500\n",
      "loss: 1.5504 ; train dice : 1.4496 ; val dice: 1.1614; time: 10.4683s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 287/500\n",
      "loss: 1.5502 ; train dice : 1.4498 ; val dice: 1.1479; time: 10.4519s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 288/500\n",
      "loss: 1.5474 ; train dice : 1.4526 ; val dice: 1.1798; time: 10.4569s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 289/500\n",
      "loss: 1.5488 ; train dice : 1.4512 ; val dice: 1.1442; time: 10.4576s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 290/500\n",
      "loss: 1.5473 ; train dice : 1.4527 ; val dice: 1.1554; time: 10.6211s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 291/500\n",
      "loss: 1.5481 ; train dice : 1.4519 ; val dice: 1.1522; time: 10.4772s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 292/500\n",
      "loss: 1.5469 ; train dice : 1.4531 ; val dice: 1.1717; time: 10.4416s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 293/500\n",
      "loss: 1.5455 ; train dice : 1.4545 ; val dice: 1.1539; time: 10.4743s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 294/500\n",
      "loss: 1.5443 ; train dice : 1.4557 ; val dice: 1.1594; time: 10.4847s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 295/500\n",
      "loss: 1.5424 ; train dice : 1.4576 ; val dice: 1.1660; time: 10.4794s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 296/500\n",
      "loss: 1.5427 ; train dice : 1.4573 ; val dice: 1.1746; time: 10.5404s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 297/500\n",
      "loss: 1.5426 ; train dice : 1.4574 ; val dice: 1.1584; time: 10.5250s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 298/500\n",
      "loss: 1.5439 ; train dice : 1.4561 ; val dice: 1.1693; time: 10.4700s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 299/500\n",
      "loss: 1.5440 ; train dice : 1.4560 ; val dice: 1.1529; time: 10.4513s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 300/500\n",
      "loss: 1.5472 ; train dice : 1.4528 ; val dice: 1.1624; time: 10.4569s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 301/500\n",
      "loss: 1.5483 ; train dice : 1.4517 ; val dice: 1.1700; time: 10.4560s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 302/500\n",
      "loss: 1.5476 ; train dice : 1.4524 ; val dice: 1.1500; time: 10.4544s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 303/500\n",
      "loss: 1.5489 ; train dice : 1.4511 ; val dice: 1.1101; time: 10.4522s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 304/500\n",
      "loss: 1.5579 ; train dice : 1.4421 ; val dice: 1.1539; time: 10.4379s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 305/500\n",
      "loss: 1.5469 ; train dice : 1.4531 ; val dice: 1.1679; time: 10.4606s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 306/500\n",
      "loss: 1.5431 ; train dice : 1.4569 ; val dice: 1.1719; time: 10.4449s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 307/500\n",
      "loss: 1.5416 ; train dice : 1.4584 ; val dice: 1.1774; time: 10.4613s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 308/500\n",
      "loss: 1.5434 ; train dice : 1.4566 ; val dice: 1.1680; time: 10.4582s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 309/500\n",
      "loss: 1.5445 ; train dice : 1.4555 ; val dice: 1.1756; time: 10.4793s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 310/500\n",
      "loss: 1.5437 ; train dice : 1.4563 ; val dice: 1.1562; time: 10.4539s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 311/500\n",
      "loss: 1.5463 ; train dice : 1.4537 ; val dice: 1.1667; time: 10.4610s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 312/500\n",
      "loss: 1.5451 ; train dice : 1.4549 ; val dice: 1.1627; time: 10.4446s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 313/500\n",
      "loss: 1.5515 ; train dice : 1.4485 ; val dice: 1.1632; time: 10.4552s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 314/500\n",
      "loss: 1.5510 ; train dice : 1.4490 ; val dice: 1.1506; time: 10.4409s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 315/500\n",
      "loss: 1.5454 ; train dice : 1.4546 ; val dice: 1.1785; time: 10.4519s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 316/500\n",
      "loss: 1.5411 ; train dice : 1.4589 ; val dice: 1.1534; time: 10.4366s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 317/500\n",
      "loss: 1.5406 ; train dice : 1.4594 ; val dice: 1.1746; time: 10.4591s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 318/500\n",
      "loss: 1.5415 ; train dice : 1.4585 ; val dice: 1.1565; time: 10.4510s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 319/500\n",
      "loss: 1.5430 ; train dice : 1.4570 ; val dice: 1.1691; time: 10.4541s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 320/500\n",
      "loss: 1.5473 ; train dice : 1.4527 ; val dice: 1.1435; time: 10.4602s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 321/500\n",
      "loss: 1.5467 ; train dice : 1.4533 ; val dice: 1.1543; time: 10.4494s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 322/500\n",
      "loss: 1.5434 ; train dice : 1.4566 ; val dice: 1.1626; time: 10.4268s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 323/500\n",
      "loss: 1.5397 ; train dice : 1.4603 ; val dice: 1.1333; time: 10.4307s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 324/500\n",
      "loss: 1.5398 ; train dice : 1.4602 ; val dice: 1.1330; time: 10.4204s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 325/500\n",
      "loss: 1.5417 ; train dice : 1.4583 ; val dice: 1.1755; time: 10.4343s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 326/500\n",
      "loss: 1.5442 ; train dice : 1.4558 ; val dice: 1.1526; time: 10.4435s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 327/500\n",
      "loss: 1.5452 ; train dice : 1.4548 ; val dice: 1.1437; time: 10.4323s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 328/500\n",
      "loss: 1.5492 ; train dice : 1.4508 ; val dice: 1.1741; time: 10.4247s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 329/500\n",
      "loss: 1.5488 ; train dice : 1.4512 ; val dice: 1.1648; time: 10.4365s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 330/500\n",
      "loss: 1.5522 ; train dice : 1.4478 ; val dice: 1.1551; time: 10.4523s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 331/500\n",
      "loss: 1.5453 ; train dice : 1.4547 ; val dice: 1.1716; time: 10.4363s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 332/500\n",
      "loss: 1.5406 ; train dice : 1.4594 ; val dice: 1.1670; time: 10.4467s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 333/500\n",
      "loss: 1.5401 ; train dice : 1.4599 ; val dice: 1.1693; time: 10.4715s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 334/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1754; time: 10.4510s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 335/500\n",
      "loss: 1.5376 ; train dice : 1.4624 ; val dice: 1.1812; time: 10.4494s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 336/500\n",
      "loss: 1.5370 ; train dice : 1.4630 ; val dice: 1.1841; time: 10.4598s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 337/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1878; time: 10.4560s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 338/500\n",
      "loss: 1.5391 ; train dice : 1.4609 ; val dice: 1.1720; time: 10.4564s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 339/500\n",
      "loss: 1.5386 ; train dice : 1.4614 ; val dice: 1.1742; time: 10.4520s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 340/500\n",
      "loss: 1.5381 ; train dice : 1.4619 ; val dice: 1.1751; time: 10.4633s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 341/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1749; time: 10.4971s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 342/500\n",
      "loss: 1.5395 ; train dice : 1.4605 ; val dice: 1.1572; time: 10.4750s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 343/500\n",
      "loss: 1.5407 ; train dice : 1.4593 ; val dice: 1.1704; time: 10.4740s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 344/500\n",
      "loss: 1.5403 ; train dice : 1.4597 ; val dice: 1.1893; time: 10.4832s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 345/500\n",
      "loss: 1.5447 ; train dice : 1.4553 ; val dice: 1.1650; time: 10.4620s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 346/500\n",
      "loss: 1.5428 ; train dice : 1.4572 ; val dice: 1.1613; time: 10.4741s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 347/500\n",
      "loss: 1.5408 ; train dice : 1.4592 ; val dice: 1.1791; time: 10.3636s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 348/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1605; time: 10.3525s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 349/500\n",
      "loss: 1.5374 ; train dice : 1.4626 ; val dice: 1.1663; time: 10.3612s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 350/500\n",
      "loss: 1.5398 ; train dice : 1.4602 ; val dice: 1.1647; time: 10.3680s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 351/500\n",
      "loss: 1.5401 ; train dice : 1.4599 ; val dice: 1.1686; time: 10.3522s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 352/500\n",
      "loss: 1.5409 ; train dice : 1.4591 ; val dice: 1.1678; time: 10.3621s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 353/500\n",
      "loss: 1.5420 ; train dice : 1.4580 ; val dice: 1.1696; time: 10.3534s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 354/500\n",
      "loss: 1.5478 ; train dice : 1.4522 ; val dice: 1.1672; time: 10.3563s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 355/500\n",
      "loss: 1.5448 ; train dice : 1.4552 ; val dice: 1.1519; time: 10.3609s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 356/500\n",
      "loss: 1.5415 ; train dice : 1.4585 ; val dice: 1.1691; time: 10.3693s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 357/500\n",
      "loss: 1.5376 ; train dice : 1.4624 ; val dice: 1.1624; time: 10.4023s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 358/500\n",
      "loss: 1.5363 ; train dice : 1.4637 ; val dice: 1.1612; time: 10.4564s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 359/500\n",
      "loss: 1.5453 ; train dice : 1.4547 ; val dice: 1.1437; time: 10.4656s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 360/500\n",
      "loss: 1.5493 ; train dice : 1.4507 ; val dice: 1.1644; time: 10.4681s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 361/500\n",
      "loss: 1.5427 ; train dice : 1.4573 ; val dice: 1.1715; time: 10.4856s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 362/500\n",
      "loss: 1.5385 ; train dice : 1.4615 ; val dice: 1.1614; time: 10.4520s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 363/500\n",
      "loss: 1.5356 ; train dice : 1.4644 ; val dice: 1.1679; time: 10.4471s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 364/500\n",
      "loss: 1.5350 ; train dice : 1.4650 ; val dice: 1.1728; time: 10.4602s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 365/500\n",
      "loss: 1.5350 ; train dice : 1.4650 ; val dice: 1.1830; time: 10.4641s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 366/500\n",
      "loss: 1.5354 ; train dice : 1.4646 ; val dice: 1.1634; time: 10.4682s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 367/500\n",
      "loss: 1.5359 ; train dice : 1.4641 ; val dice: 1.1592; time: 10.4583s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 368/500\n",
      "loss: 1.5352 ; train dice : 1.4648 ; val dice: 1.1625; time: 10.4821s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 369/500\n",
      "loss: 1.5394 ; train dice : 1.4606 ; val dice: 1.1769; time: 10.4604s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 370/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1713; time: 10.4587s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 371/500\n",
      "loss: 1.5378 ; train dice : 1.4622 ; val dice: 1.1621; time: 10.4478s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 372/500\n",
      "loss: 1.5402 ; train dice : 1.4598 ; val dice: 1.1520; time: 10.4570s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 373/500\n",
      "loss: 1.5409 ; train dice : 1.4591 ; val dice: 1.1485; time: 10.4378s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 374/500\n",
      "loss: 1.5401 ; train dice : 1.4599 ; val dice: 1.1025; time: 10.4409s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 375/500\n",
      "loss: 1.5387 ; train dice : 1.4613 ; val dice: 1.1443; time: 10.4647s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 376/500\n",
      "loss: 1.5372 ; train dice : 1.4628 ; val dice: 1.1669; time: 10.4608s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 377/500\n",
      "loss: 1.5369 ; train dice : 1.4631 ; val dice: 1.1498; time: 10.4637s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 378/500\n",
      "loss: 1.5351 ; train dice : 1.4649 ; val dice: 1.1585; time: 10.4684s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 379/500\n",
      "loss: 1.5369 ; train dice : 1.4631 ; val dice: 1.1643; time: 10.5010s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 380/500\n",
      "loss: 1.5363 ; train dice : 1.4637 ; val dice: 1.1695; time: 10.4769s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 381/500\n",
      "loss: 1.5362 ; train dice : 1.4638 ; val dice: 1.1629; time: 10.4523s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 382/500\n",
      "loss: 1.5356 ; train dice : 1.4644 ; val dice: 1.1556; time: 10.4531s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 383/500\n",
      "loss: 1.5351 ; train dice : 1.4649 ; val dice: 1.1677; time: 10.4726s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 384/500\n",
      "loss: 1.5355 ; train dice : 1.4645 ; val dice: 1.1582; time: 10.4686s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 385/500\n",
      "loss: 1.5362 ; train dice : 1.4638 ; val dice: 1.1610; time: 10.4864s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 386/500\n",
      "loss: 1.5366 ; train dice : 1.4634 ; val dice: 1.1625; time: 10.4376s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 387/500\n",
      "loss: 1.5376 ; train dice : 1.4624 ; val dice: 1.1369; time: 10.4831s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 388/500\n",
      "loss: 1.5386 ; train dice : 1.4614 ; val dice: 1.1673; time: 10.4820s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 389/500\n",
      "loss: 1.5401 ; train dice : 1.4599 ; val dice: 1.1535; time: 10.4593s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 390/500\n",
      "loss: 1.5401 ; train dice : 1.4599 ; val dice: 1.1417; time: 10.4654s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 391/500\n",
      "loss: 1.5500 ; train dice : 1.4500 ; val dice: 1.1522; time: 10.4705s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 392/500\n",
      "loss: 1.5488 ; train dice : 1.4512 ; val dice: 1.0725; time: 10.4787s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 393/500\n",
      "loss: 1.5489 ; train dice : 1.4511 ; val dice: 1.1677; time: 10.4721s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 394/500\n",
      "loss: 1.5490 ; train dice : 1.4510 ; val dice: 1.1493; time: 10.4734s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 395/500\n",
      "loss: 1.5403 ; train dice : 1.4597 ; val dice: 1.1459; time: 10.4647s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 396/500\n",
      "loss: 1.5364 ; train dice : 1.4636 ; val dice: 1.1548; time: 10.4902s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 397/500\n",
      "loss: 1.5339 ; train dice : 1.4661 ; val dice: 1.1852; time: 10.4276s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 398/500\n",
      "loss: 1.5317 ; train dice : 1.4683 ; val dice: 1.1880; time: 10.4247s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 399/500\n",
      "loss: 1.5323 ; train dice : 1.4677 ; val dice: 1.1811; time: 10.4286s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 400/500\n",
      "loss: 1.5318 ; train dice : 1.4682 ; val dice: 1.1944; time: 10.4342s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 401/500\n",
      "loss: 1.5319 ; train dice : 1.4681 ; val dice: 1.1932; time: 10.4372s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 402/500\n",
      "loss: 1.5320 ; train dice : 1.4680 ; val dice: 1.1733; time: 10.4480s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 403/500\n",
      "loss: 1.5342 ; train dice : 1.4658 ; val dice: 1.1930; time: 10.4674s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 404/500\n",
      "loss: 1.5353 ; train dice : 1.4647 ; val dice: 1.1837; time: 10.4526s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 405/500\n",
      "loss: 1.5354 ; train dice : 1.4646 ; val dice: 1.1718; time: 10.4827s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 406/500\n",
      "loss: 1.5336 ; train dice : 1.4664 ; val dice: 1.1808; time: 10.4463s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 407/500\n",
      "loss: 1.5341 ; train dice : 1.4659 ; val dice: 1.1793; time: 10.4325s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 408/500\n",
      "loss: 1.5330 ; train dice : 1.4670 ; val dice: 1.1909; time: 10.4103s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 409/500\n",
      "loss: 1.5348 ; train dice : 1.4652 ; val dice: 1.1861; time: 10.4644s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 410/500\n",
      "loss: 1.5372 ; train dice : 1.4628 ; val dice: 1.1898; time: 10.4308s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 411/500\n",
      "loss: 1.5373 ; train dice : 1.4627 ; val dice: 1.1903; time: 10.4488s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 412/500\n",
      "loss: 1.5386 ; train dice : 1.4614 ; val dice: 1.1695; time: 10.5055s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 413/500\n",
      "loss: 1.5382 ; train dice : 1.4618 ; val dice: 1.1819; time: 10.4293s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 414/500\n",
      "loss: 1.5379 ; train dice : 1.4621 ; val dice: 1.1894; time: 10.4092s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 415/500\n",
      "loss: 1.5343 ; train dice : 1.4657 ; val dice: 1.1704; time: 10.4300s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 416/500\n",
      "loss: 1.5335 ; train dice : 1.4665 ; val dice: 1.1742; time: 10.4258s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 417/500\n",
      "loss: 1.5331 ; train dice : 1.4669 ; val dice: 1.1685; time: 10.4896s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 418/500\n",
      "loss: 1.5335 ; train dice : 1.4665 ; val dice: 1.1774; time: 10.4442s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 419/500\n",
      "loss: 1.5334 ; train dice : 1.4666 ; val dice: 1.1912; time: 10.5681s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 420/500\n",
      "loss: 1.5330 ; train dice : 1.4670 ; val dice: 1.1875; time: 10.4695s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 421/500\n",
      "loss: 1.5335 ; train dice : 1.4665 ; val dice: 1.1716; time: 10.4588s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 422/500\n",
      "loss: 1.5337 ; train dice : 1.4663 ; val dice: 1.1764; time: 10.3485s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 423/500\n",
      "loss: 1.5343 ; train dice : 1.4657 ; val dice: 1.1780; time: 10.3321s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 424/500\n",
      "loss: 1.5351 ; train dice : 1.4649 ; val dice: 1.1758; time: 10.3441s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 425/500\n",
      "loss: 1.5345 ; train dice : 1.4655 ; val dice: 1.1733; time: 10.3333s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 426/500\n",
      "loss: 1.5368 ; train dice : 1.4632 ; val dice: 1.1631; time: 10.4133s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 427/500\n",
      "loss: 1.5425 ; train dice : 1.4575 ; val dice: 1.1642; time: 10.4097s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 428/500\n",
      "loss: 1.5436 ; train dice : 1.4564 ; val dice: 1.1604; time: 10.4330s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 429/500\n",
      "loss: 1.5397 ; train dice : 1.4603 ; val dice: 1.1832; time: 10.6090s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 430/500\n",
      "loss: 1.5352 ; train dice : 1.4648 ; val dice: 1.1743; time: 10.5195s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 431/500\n",
      "loss: 1.5333 ; train dice : 1.4667 ; val dice: 1.1842; time: 10.4808s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 432/500\n",
      "loss: 1.5331 ; train dice : 1.4669 ; val dice: 1.1800; time: 10.4540s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 433/500\n",
      "loss: 1.5321 ; train dice : 1.4679 ; val dice: 1.1852; time: 10.4636s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 434/500\n",
      "loss: 1.5314 ; train dice : 1.4686 ; val dice: 1.1806; time: 10.4374s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 435/500\n",
      "loss: 1.5327 ; train dice : 1.4673 ; val dice: 1.1729; time: 10.4522s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 436/500\n",
      "loss: 1.5326 ; train dice : 1.4674 ; val dice: 1.1760; time: 10.4704s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 437/500\n",
      "loss: 1.5315 ; train dice : 1.4685 ; val dice: 1.1669; time: 10.4184s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 438/500\n",
      "loss: 1.5323 ; train dice : 1.4677 ; val dice: 1.1729; time: 10.4606s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 439/500\n",
      "loss: 1.5337 ; train dice : 1.4663 ; val dice: 1.1652; time: 10.4679s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 440/500\n",
      "loss: 1.5347 ; train dice : 1.4653 ; val dice: 1.1694; time: 10.4957s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 441/500\n",
      "loss: 1.5339 ; train dice : 1.4661 ; val dice: 1.1914; time: 10.4615s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 442/500\n",
      "loss: 1.5327 ; train dice : 1.4673 ; val dice: 1.1803; time: 10.4623s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 443/500\n",
      "loss: 1.5316 ; train dice : 1.4684 ; val dice: 1.1731; time: 10.4656s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 444/500\n",
      "loss: 1.5333 ; train dice : 1.4667 ; val dice: 1.1783; time: 10.4480s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 445/500\n",
      "loss: 1.5325 ; train dice : 1.4675 ; val dice: 1.1699; time: 10.4417s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 446/500\n",
      "loss: 1.5345 ; train dice : 1.4655 ; val dice: 1.1394; time: 10.4563s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 447/500\n",
      "loss: 1.5333 ; train dice : 1.4667 ; val dice: 1.1610; time: 10.4555s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 448/500\n",
      "loss: 1.5329 ; train dice : 1.4671 ; val dice: 1.1677; time: 10.4463s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 449/500\n",
      "loss: 1.5337 ; train dice : 1.4663 ; val dice: 1.1695; time: 10.4319s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 450/500\n",
      "loss: 1.5316 ; train dice : 1.4684 ; val dice: 1.1536; time: 10.4555s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 451/500\n",
      "loss: 1.5340 ; train dice : 1.4660 ; val dice: 1.1652; time: 10.4525s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 452/500\n",
      "loss: 1.5328 ; train dice : 1.4672 ; val dice: 1.1768; time: 10.4305s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 453/500\n",
      "loss: 1.5314 ; train dice : 1.4686 ; val dice: 1.1773; time: 10.4486s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 454/500\n",
      "loss: 1.5317 ; train dice : 1.4683 ; val dice: 1.1809; time: 10.4512s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 455/500\n",
      "loss: 1.5319 ; train dice : 1.4681 ; val dice: 1.1713; time: 10.4746s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 456/500\n",
      "loss: 1.5315 ; train dice : 1.4685 ; val dice: 1.1673; time: 10.4439s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 457/500\n",
      "loss: 1.5315 ; train dice : 1.4685 ; val dice: 1.1593; time: 10.4761s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 458/500\n",
      "loss: 1.5342 ; train dice : 1.4658 ; val dice: 1.1711; time: 10.4212s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 459/500\n",
      "loss: 1.5368 ; train dice : 1.4632 ; val dice: 1.1814; time: 10.4665s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 460/500\n",
      "loss: 1.5352 ; train dice : 1.4648 ; val dice: 1.1569; time: 10.4836s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 461/500\n",
      "loss: 1.5379 ; train dice : 1.4621 ; val dice: 1.1733; time: 10.4837s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 462/500\n",
      "loss: 1.5392 ; train dice : 1.4608 ; val dice: 1.1535; time: 10.4687s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 463/500\n",
      "loss: 1.5404 ; train dice : 1.4596 ; val dice: 1.1613; time: 10.4690s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 464/500\n",
      "loss: 1.5388 ; train dice : 1.4612 ; val dice: 1.1487; time: 10.4456s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 465/500\n",
      "loss: 1.5358 ; train dice : 1.4642 ; val dice: 1.1755; time: 10.4681s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 466/500\n",
      "loss: 1.5319 ; train dice : 1.4681 ; val dice: 1.1734; time: 10.4311s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 467/500\n",
      "loss: 1.5294 ; train dice : 1.4706 ; val dice: 1.1745; time: 10.4443s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 468/500\n",
      "loss: 1.5280 ; train dice : 1.4720 ; val dice: 1.1891; time: 10.4411s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 469/500\n",
      "loss: 1.5290 ; train dice : 1.4710 ; val dice: 1.1685; time: 10.4627s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 470/500\n",
      "loss: 1.5291 ; train dice : 1.4709 ; val dice: 1.1739; time: 10.4296s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 471/500\n",
      "loss: 1.5291 ; train dice : 1.4709 ; val dice: 1.1761; time: 10.4399s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 472/500\n",
      "loss: 1.5290 ; train dice : 1.4710 ; val dice: 1.1809; time: 10.4309s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 473/500\n",
      "loss: 1.5304 ; train dice : 1.4696 ; val dice: 1.1663; time: 10.4518s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 474/500\n",
      "loss: 1.5311 ; train dice : 1.4689 ; val dice: 1.1775; time: 10.4517s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 475/500\n",
      "loss: 1.5310 ; train dice : 1.4690 ; val dice: 1.1695; time: 10.4401s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 476/500\n",
      "loss: 1.5313 ; train dice : 1.4687 ; val dice: 1.1653; time: 10.4393s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 477/500\n",
      "loss: 1.5313 ; train dice : 1.4687 ; val dice: 1.1654; time: 10.4512s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 478/500\n",
      "loss: 1.5316 ; train dice : 1.4684 ; val dice: 1.1817; time: 10.4711s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 479/500\n",
      "loss: 1.5306 ; train dice : 1.4694 ; val dice: 1.1873; time: 10.4383s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 480/500\n",
      "loss: 1.5306 ; train dice : 1.4694 ; val dice: 1.1638; time: 10.4339s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 481/500\n",
      "loss: 1.5313 ; train dice : 1.4687 ; val dice: 1.1548; time: 10.4493s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 482/500\n",
      "loss: 1.5319 ; train dice : 1.4681 ; val dice: 1.1661; time: 10.4682s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 483/500\n",
      "loss: 1.5306 ; train dice : 1.4694 ; val dice: 1.1740; time: 10.4716s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 484/500\n",
      "loss: 1.5304 ; train dice : 1.4696 ; val dice: 1.1484; time: 10.4755s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 485/500\n",
      "loss: 1.5309 ; train dice : 1.4691 ; val dice: 1.1694; time: 10.4688s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 486/500\n",
      "loss: 1.5314 ; train dice : 1.4686 ; val dice: 1.1772; time: 10.5118s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 487/500\n",
      "loss: 1.5326 ; train dice : 1.4674 ; val dice: 1.1632; time: 10.4904s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 488/500\n",
      "loss: 1.5336 ; train dice : 1.4664 ; val dice: 1.1650; time: 10.4810s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 489/500\n",
      "loss: 1.5345 ; train dice : 1.4655 ; val dice: 1.1674; time: 10.4444s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 490/500\n",
      "loss: 1.5337 ; train dice : 1.4663 ; val dice: 1.1732; time: 10.4762s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 491/500\n",
      "loss: 1.5335 ; train dice : 1.4665 ; val dice: 1.1681; time: 10.4645s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 492/500\n",
      "loss: 1.5329 ; train dice : 1.4671 ; val dice: 1.1579; time: 10.5049s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 493/500\n",
      "loss: 1.5317 ; train dice : 1.4683 ; val dice: 1.1539; time: 10.4764s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 494/500\n",
      "loss: 1.5309 ; train dice : 1.4691 ; val dice: 1.1646; time: 10.4662s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 495/500\n",
      "loss: 1.5304 ; train dice : 1.4696 ; val dice: 1.1654; time: 10.4698s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 496/500\n",
      "loss: 1.5312 ; train dice : 1.4688 ; val dice: 1.1489; time: 10.4524s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 497/500\n",
      "loss: 1.5364 ; train dice : 1.4636 ; val dice: 1.1578; time: 10.4674s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 498/500\n",
      "loss: 1.5373 ; train dice : 1.4627 ; val dice: 1.1344; time: 10.4426s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 499/500\n",
      "loss: 1.5428 ; train dice : 1.4572 ; val dice: 1.1477; time: 10.4504s\n",
      "Best loss : 1.1980 at epoch 229\n",
      "Epoch: 500/500\n",
      "loss: 1.5395 ; train dice : 1.4605 ; val dice: 1.1695; time: 10.4557s\n",
      "Best loss : 1.1980 at epoch 229\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    epoch_start = time.time()\n",
    "    net.train()\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, 500))\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    for image, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = net(image)\n",
    "        loss = loss_func(pred,label)\n",
    "        train_loss += loss.item() * image.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = 3 - train_loss/len(train_set)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for image,label in test_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = net(image)\n",
    "            loss = loss_func(pred,label)\n",
    "            val_loss += loss.item() * image.shape[0]\n",
    "    avg_val_loss = 3 - val_loss/len(test_set)\n",
    "    if avg_val_loss > best_dice:\n",
    "            best_dice = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(net.state_dict(), 'best_model.pth')\n",
    "    epoch_end = time.time()\n",
    "    print('loss: {:.4f} ; train dice : {:.4f} ; val dice: {:.4f}; time: {:.4f}s'.format(3 - avg_train_loss, avg_train_loss, avg_val_loss, epoch_end-epoch_start))\n",
    "    print(\"Best loss : {:.4f} at epoch {:03d}\".format(best_dice, best_epoch))\n",
    "    history.append(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history)\n",
    "plt.legend(['Tr Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1.5778248310089111\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.76 GiB total capacity; 8.79 GiB already allocated; 249.19 MiB free; 9.57 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73df07376bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dong/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f6db41bb6f59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dong/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-440288b2dc70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.76 GiB total capacity; 8.79 GiB already allocated; 249.19 MiB free; 9.57 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "net.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "i = 0\n",
    "for image, label in train_loader:\n",
    "    print(label.shape[1])\n",
    "    image = image.to(device=device, dtype=torch.float32)\n",
    "    label = label.to(device=device, dtype=torch.float32)\n",
    "    pred = net(image)\n",
    "    loss = loss_func(pred,label)\n",
    "    print(loss.item())\n",
    "    pred = np.array(pred.data.cpu()[0])\n",
    "    label = np.array(label.data.cpu()[0])\n",
    "    pred[pred >= 0.5] = 255\n",
    "    pred[pred < 0.5] = 0\n",
    "    i+=1\n",
    "    pred = pred[0]\n",
    "    label = label[0]\n",
    "    #pred = pred.transpose(1,2,0)\n",
    "    #label = label.transpose(1,2,0)\n",
    "    imageio.imwrite('./res/' + str(i) + '.png', pred)\n",
    "    imageio.imwrite('./res/' + str(i) + '_label.png', label*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
